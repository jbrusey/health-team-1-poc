# Proof of concept for Health Team 1

Proof-of-concept system to provide tumour advice to clinicians for the treatment of breast cancer.

## Project structure

```
app/
  __init__.py        # Flask application factory
  forms.py           # WTForms definitions for the intake workflow
  routes.py          # HTTP routes and view logic
  templates/         # Jinja templates for the UI
pyproject.toml       # Project metadata and dependency definitions
run.py               # Development server entrypoint
uv.lock              # Resolved dependency lockfile generated by uv
```

Additional folders such as `data/` and `vector_store/` will be created in later steps to support retrieval-augmented generation.

## Getting started

1. Install [uv](https://docs.astral.sh/uv/) if you don't already have it available.
2. Create and activate a Python 3.11 virtual environment (optionalâ€”`uv` can also manage an isolated environment for you).
3. Install dependencies and create the `.venv` managed by `uv`:
   ```bash
   uv sync
   ```
4. Copy the environment template and populate secrets:
   ```bash
   cp .env.example .env
   ```
5. Run the development server:
   ```bash
   uv run flask --app run:app --debug run
   ```

The default server listens on http://127.0.0.1:5000/ and exposes a preliminary form for capturing EHR, pathology, and genomic details. A dedicated "LLM playground" page at `/llm` lets you send arbitrary prompts to the configured language model so you can experiment with the integration before wiring it into the intake workflow. A companion "Multi-agent" view at `/llm/multi` runs the same prompt through multiple Ollama agents so you can compare responses side-by-side.

### LLM configuration

LLM providers are driven by environment variables so you can switch between local Ollama instances and hosted APIs without touching the code:

| Variable | Purpose | Default |
| --- | --- | --- |
| `LLM_DEFAULT_PROVIDER` | Provider used when no override is chosen (`ollama` or `openai`). | `ollama` |
| `LLM_OLLAMA_HOST` / `LLM_OLLAMA_PORT` / `LLM_OLLAMA_SCHEME` | Connection info for the Ollama server. | `localhost` / `11434` / `http` |
| `LLM_OLLAMA_MODEL` | Default Ollama model name (e.g., `llama3`, `mistral`). | `llama3` |
| `LLM_OLLAMA_OPTIONS` | Optional JSON encoded dictionary of model options to pass through. | unset |
| `LLM_MULTI_AGENT_PORTS` | Comma-separated list of Ollama ports to query in the multi-agent view. | `11434,11435` |
| `LLM_OPENAI_MODEL` / `LLM_OPENAI_URL` / `LLM_OPENAI_TEMPERATURE` | Parameters for the ChatGPT-compatible endpoint. Requires `OPENAI_API_KEY`. | `gpt-3.5-turbo` / OpenAI chat completions URL / unset |
| `LLM_REQUEST_TIMEOUT` | Request timeout in seconds for every provider. | `60` |

Set these in `.env` to match your local Ollama hostname/port, preferred model, or to direct traffic to a different API such as ChatGPT.
